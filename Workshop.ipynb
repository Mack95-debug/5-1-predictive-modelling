{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Face Recognition, but not evil this time\n",
    "\n",
    "Using the faces dataset in:\n",
    "\n",
    "```\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "```\n",
    "\n",
    "If you use the `faces.target` and `faces.target_names` attributes, you can build a facial recognition algorithm.\n",
    "\n",
    "Use sklearn **gridsearch** (or an equivalent, like random search) to optimize the model for accuracy. Try both a SVM-based classifier and a logistic regression based classifier (with a feature pipeline of your choice) to get the best model. You should have at least 80% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "X = faces.data\n",
    "y = faces.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comps = 150\n",
    "\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pca', PCA(n_components = n_comps, svd_solver ='randomized',\n",
    "            whiten = True)),\n",
    "    ('std', StandardScaler()),\n",
    "    ('clf', GridSearchCV(SVC(kernel ='rbf', class_weight ='balanced'), param_grid))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=150,\n",
       "                     random_state=None, svd_solver='randomized', tol=0.0,\n",
       "                     whiten=True)),\n",
       "                ('std',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('clf',\n",
       "                 GridSearchCV(cv=None, error_score=nan,\n",
       "                              estimator=SVC(C=1.0, break_ties=False,\n",
       "                                            cache_size=200,\n",
       "                                            class_weight='balanced', coef0=0.0,\n",
       "                                            decis...\n",
       "                                            degree=3, gamma='scale',\n",
       "                                            kernel='rbf', max_iter=-1,\n",
       "                                            probability=False,\n",
       "                                            random_state=None, shrinking=True,\n",
       "                                            tol=0.001, verbose=False),\n",
       "                              iid='deprecated', n_jobs=None,\n",
       "                              param_grid={'C': [1000.0, 5000.0, 10000.0,\n",
       "                                                50000.0, 100000.0],\n",
       "                                          'gamma': [0.0001, 0.0005, 0.001,\n",
       "                                                    0.005, 0.01, 0.1]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8516320474777448"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Bag of Words, Bag of Popcorn\n",
    "\n",
    "By this point, you are ready for the [Bag of Words, Bag of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) competition. \n",
    "\n",
    "Use NLP feature pre-processing (using, SKLearn, Gensim, Spacy or Hugginface) to build the best classifier you can. Use a  feature pipeline, and gridsearch for your final model.\n",
    "\n",
    "A succesful project should get 90% or more on a **holdout** dataset you kept for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/kalebmckenzie/Documents/GitHub/5-1-predictive-modelling/labeledTrainData.tsv', sep='\\t')\n",
    "df = df.drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data and text cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "def remove_html(text):\n",
    "    bs = BeautifulSoup(text, \"html.parser\")\n",
    "    return ' ' + bs.get_text() + ' '\n",
    "\n",
    "df['review'] = df['review'].apply(remove_html)\n",
    "\n",
    "def keep_only_letters(text):\n",
    "    text=re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(keep_only_letters)\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['review'] = df['review'].apply(convert_to_lowercase)\n",
    "\n",
    "def clean_reviews(text):\n",
    "    text = remove_html(text)\n",
    "    text = keep_only_letters(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    for stopword in english_stop_words:\n",
    "        stopword = ' ' + stopword + ' '\n",
    "        text = text.replace(stopword, ' ')\n",
    "    return text\n",
    " \n",
    "df['review'] = df['review'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stemming(text):\n",
    "    stemmer = nltk.porter.PorterStemmer()\n",
    "    stemmed = ' '.join([stemmer.stem(token) for token in text.split()])\n",
    "    return stemmed\n",
    " \n",
    "df['review'] = df['review'].apply(text_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into a training set and test set\n",
    "X_train = df[:20000]\n",
    "y_test = df[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(20000, 1447824) (5000, 1447824)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(binary=False,ngram_range=(1,2))\n",
    "\n",
    "tf_features_train = vectorizer.fit_transform(X_train['review'])\n",
    "\n",
    "tf_features_test = vectorizer.transform(y_test['review'])\n",
    "\n",
    "print (tf_features_train.shape, tf_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20000 5000\n"
     ]
    }
   ],
   "source": [
    "train_labels = [1 if sentiment== 1 else 0 for sentiment in X_train['sentiment']]\n",
    "test_labels = [1 if sentiment== 1 else 0 for sentiment in y_test['sentiment']]\n",
    "print (len(train_labels), len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipes = Pipeline([\n",
    "    ('reg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/kalebmckenzie/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('reg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "pipes.fit(tf_features_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.89      0.88      0.88      2472\n           1       0.88      0.89      0.89      2528\n\n    accuracy                           0.89      5000\n   macro avg       0.89      0.89      0.89      5000\nweighted avg       0.89      0.89      0.89      5000\n\n[[2176  296]\n [ 274 2254]]\n0.886\n"
     ]
    }
   ],
   "source": [
    "predictions = pipes.predict(tf_features_test)\n",
    "print(sklearn.metrics.classification_report(test_labels, predictions))\n",
    "print(sklearn.metrics.confusion_matrix(test_labels, predictions, labels=[0, 1]))\n",
    "print(accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.886\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}